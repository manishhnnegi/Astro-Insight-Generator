Hereâ€™s a polished **README.md** for your **Astro-Insight-Generator** project based on your directory and assignment content:

```markdown
# Astro-Insight-Generator

A personalized **Astrological Insight Generator** that provides daily astrological predictions based on the user's birth details. This service combines zodiac logic and LLM-based language generation, with optional multilingual support and caching.

---

## ğŸ“ Problem Statement

This project takes a user's birth details (name, date, time, and location) and returns a personalized daily astrological insight. The system:

- Infers zodiac signs from birth date/time.
- Uses simplified or dummy astrology rules per zodiac.
- Generates natural language insights using a dummy or LLM-based pipeline.
- Supports multilingual outputs (e.g., Hindi) via translation modules.
- Implements caching to store and retrieve previous predictions.

---

## ğŸ¯ Features

- **Zodiac Inference:** Calculates the user's zodiac sign from birth date.
- **LLM/Dummy Prediction:** Generates personalized insights.
- **Translation:** Supports multiple languages via Google Translate or dummy translations.
- **Caching:** Stores previous predictions for quick retrieval.
- **REST API:** Exposes a `/predict` endpoint for easy integration.

---

## ğŸ§± Project Structure

```

Astro-Insight-Generator/
â”‚
â”œâ”€â”€ app.py                     # Entry point to launch the app
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.py              # Configuration (models, server, API keys)
â”‚   â””â”€â”€ .env                   # Store GEMINI\_API\_KEY
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ interface/
â”‚   â”‚   â”œâ”€â”€ ui\_main.py         # UIStarter (orchestrates app launch)
â”‚   â”‚   â””â”€â”€ ui\_backend.py      # Flask routes & backend logic
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ model\_setup.py     # Loads Gemini LLM
â”‚   â”‚   â””â”€â”€ model\_infer.py     # Prediction pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ llms/
â”‚   â”‚   â””â”€â”€ gemini\_client.py   # Gemini LLM wrapper
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ prompt.py          # Prompt templates
â”‚   â”‚
â”‚   â”œâ”€â”€ translator/
â”‚   â”‚   â””â”€â”€ translate.py       # Translation logic
â”‚   â”‚
â”‚   â””â”€â”€ cache/
â”‚       â””â”€â”€ cache.py           # Caching system
â”‚
â””â”€â”€ cache\_db/
â””â”€â”€ cache.json             # Stores cached predictions

````

---

## ğŸ“Œ Sample Input/Output

**Input:**

```json
{
  "name": "Ritika",
  "birth_date": "1995-08-20",
  "birth_time": "14:30",
  "birth_place": "Jaipur, India",
  "language": "en"
}
````

**Output:**

```json
{
  "zodiac": "Leo",
  "insight": "Your innate leadership and warmth will shine today. Embrace spontaneity and avoid overthinking.",
  "language": "en",
  "cached": false
}
```

---

## âš¡ Installation

1. Clone the repository:

```bash
git clone https://github.com/<your-username>/Astro-Insight-Generator.git
cd Astro-Insight-Generator
```

2. Create a virtual environment and activate it:

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/macOS
source venv/bin/activate
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

4. Set up environment variables:

```bash
# In config/.env
GEMINI_API_KEY=<YOUR_GOOGLE_GEMINI_API_KEY>
```

---

## ğŸš€ Running the Application

Run the app via **Flask**:

```bash
python app.py
```

By default, the API will be available at:

```
http://127.0.0.1:8000/predict
```

---

## ğŸ›  API Usage

Send a POST request with JSON payload:

```bash
curl -X POST http://127.0.0.1:8000/predict \
-H "Content-Type: application/json" \
-d '{
    "name": "Ritika",
    "birth_date": "1995-08-20",
    "birth_time": "14:30",
    "birth_place": "Jaipur, India",
    "language": "en"
}'
```

---

## âš™ Configuration

All runtime configuration is in `config/config.py`:

* `SERVER` â†’ Flask host, port, debug
* `GEMINI_API_KEY` â†’ API key for Google Gemini LLM
* `USE_DUMMY_LLM` â†’ Toggle between dummy predictor and LLM
* `USE_DUMMY_TRANSLATION` â†’ Toggle dummy translation

---

## ğŸ§© Notes on Design Choices

* **Modular Architecture:** Separate modules for zodiac calculation, translation, caching, LLM integration.
* **Dummy LLM:** Supports offline testing or early development.
* **Translation:** Supports multiple Indian languages using Google Translate or dummy translator.
* **Caching:** Stores previous predictions to improve performance and simulate personalization.

---

## ğŸ“š Future Enhancements

* Replace dummy LLM with a real Google Gemini or OpenAI model.
* Integrate real Panchang data for precise astrological insights.
* Add user authentication and history tracking.
* Expand multilingual support to include more Indian languages.
* Add scoring/personalization for predictions.

---

## ğŸ‘¨â€ğŸ’» Author

Manish Negi

---

## ğŸ“„ License

MIT License

```

---

I can also create a **short version of README** specifically for GitHub repository **homepage** if you want, which is concise and looks professional with badges, quick start, and sample output.  

Do you want me to create that version as well?
```
